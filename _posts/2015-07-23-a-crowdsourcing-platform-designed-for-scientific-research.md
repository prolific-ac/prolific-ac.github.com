---
post_author: Ekaterina Damer
layout: post
title: "A Crowdsourcing Platform Designed for Scientific Research"
description: ""
category: 
tags: [crowdsourcing, science, research]
redirect_to:
  - https://blog.prolific.ac/a-crowdsourcing-platform-designed-for-scientific-research/
---
{% include JB/setup %}

<div class="row">
	<div class="col-md-14">
 		<img class="img-responsive col-md-12" style="display: block;margin-left: auto;margin-right: auto;margin-top:40px;margin-bottom:15px;" src="/assets/img/LeanBack.jpg">
	 </div>
</div>

<p><font size="4"><br>As a junior psychological scientist I am constantly looking for ways to recruit participants. While I have used online recruitment platforms in the past with reasonable success, I’ve always mused about some of the obvious drawbacks. Why is it so hard to prescreen? How on earth do they permit this exploitative reward structure (or rather <i>lack</i> of reward structure)? Why are some still in beta after 10 years?</font></p>

<p><font size="4">Let me provide one concrete example of the limitations of current crowdsourcing solutions. Imagine you need a very specific sample, say 20-30 year old females studying engineering. How would you identify them on conventional crowdsourcing sites? You’d basically have two options: <br>A) Explain on your consent form what you're looking for, or <br> B) create filters as part of your study without giving away your criteria in advance.</font></p>

<p><font size="4">(Approaching a traditional panel company is perhaps a third option, but not really feasible for the vast majority of budget-conscious academics, especially postgraduate students.)</font></p>

<p><br><font size="4">Since A) invites dishonesty, the clever researcher will normally go for B). Yet, this is where the turmoil begins. Because the sample you're looking for (20-30 year old females studying engineering) is very specific, many participants will get kicked out of your survey after already having invested time into it. Now, if we zoom out for a second and see the bigger picture, we will quickly realise: This approach is not sustainable because it is incredibly frustrating for participants/ “workers”. In fact, it’s <i>unnecessarily</i> frustrating. What is needed is a prescreening system that determines participants’ eligibility in advance.</font></p>

<p><font size="4">This is one of the key insights that prompted us to set up a new crowdsourcing platform that is tailored for scientific research. We decided to call it, very descriptively, Prolific Academic (“Prolific”).</font></p><br>

## A fair reward structure

<p><font size="4">We agreed from the very beginning that Prolific must have a reasonable reward system, one that fairly compensates participants for their time. While taking part in scientific online experiments is fun and you feel like you actually make a meaningful contribution to advancing human knowledge, your motivation may fade over time. You will complete 1 or 2 studies hoping to win a voucher in a prize draw, but if you never win anything, you’ll eventually give up participating. This is precisely why we set the <a href="https://prolificacademic.co.uk/about/pricing">minimum reward on Prolific</a> at £5.00 (6.50€/$7.50) per hour. To us, it is a sensible, ethical reward structure and simultaneously the key to unlocking participants’ sustained motivation to participate in studies.</font></p>

<p><font size="4">In spring 2014, word came out that one of the most heavily used crowdsourcing sites, <a href="https://www.mturk.com">MTurk</a>, would close its gates for international requesters. This was right after it had stopped allowing international workers. What a blow to the international researcher community! And now, taking effect today, MTurk pricing has <a href="https://requester.mturk.com/pricing">quadrupled</a> for certain types of tasks: If you require 10+ participants for your survey, then be ready to pay a 40% commission (and an extra 5% if you use Master workers). This pricing change affects pretty much 99.99% of scientific researchers, who <a href="http://blog.prolificacademic.co.uk/2015/05/22/how-many-participants-shall-i-recruit-can-i-oversample-my-study--recruit-too-many-participants/">aim for high sample sizes & high statistical power</a>. As the newly coined hashtag #mturkgate (à la watergate) reveals, academic requesters are everything but pleased:</font></p>

<div class="row col-md-offset-2"><blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">10% changes to 20%, but also an additional 20%? mTurk, you&#39;re breaking my heart! <a href="https://twitter.com/hashtag/mturkgate?src=hash">#mturkgate</a></p>&mdash; Masha Ksendzova (@MaladaptiveMash) <a href="https://twitter.com/MaladaptiveMash/status/613531518548348928">June 24, 2015</a></blockquote></div>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<div class="row col-md-offset-2"><blockquote class="twitter-tweet" data-cards="hidden" lang="en"><p lang="en" dir="ltr">Changes to the <a href="https://twitter.com/amazonmturk">@amazonmTurk</a> commission structure unfairly target academic researchers! <a href="http://t.co/sw3cha5T3y">http://t.co/sw3cha5T3y</a> <a href="https://twitter.com/hashtag/mTurkgate?src=hash">#mTurkgate</a></p>&mdash; Justin P. Boren, PhD (@jborenSCU) <a href="https://twitter.com/jborenSCU/status/613543565839462400">June 24, 2015</a></blockquote></div>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<div class="row col-md-offset-2"><blockquote class="twitter-tweet" lang="en"><p lang="en" dir="ltr">I&#39;d rather pay workers more than pay Amazon more per worker. <a href="https://twitter.com/hashtag/mturk?src=hash">#mturk</a> <a href="https://twitter.com/hashtag/mturkgate?src=hash">#mturkgate</a> <a href="https://twitter.com/amazonmturk">@amazonmturk</a> <a href="https://t.co/xGvN1iQbiU">https://t.co/xGvN1iQbiU</a></p>&mdash; Celia Moore (@CeliaMooreLBS) <a href="https://twitter.com/CeliaMooreLBS/status/615504386035462144">June 29, 2015</a></blockquote></div>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script><br>

## Next-generation crowdsourcing

<p><font size="4">Where are we left with this? If one thing is clear, then it’s that there’s enormous untapped potential in scientific crowdsourcing. Most universities still stick to their undergraduate participant pools, refusing to share them with each other, perhaps for political or administrative reasons. (Or maybe it’s just inertia, who knows.) Whenever the capacity of using the undergraduate populations is stretched and more diverse samples are needed, academics venture out to sites like MTurk. There’s no doubt that Amazon Turk has pioneered the DIY-style crowdsourcing format. But, we suggest that it’s time to check out new alternatives that have entered the playing field, offering cost-effective, nifty solutions.</font></p>

<p><font size="4">On Prolific, we now have almost <a href="https://prolificacademic.co.uk/demographics">12,000 international participants</a> who contribute to scientific studies and earn cash rewards for themselves or for one of two chosen charities (<a href="https://www.savethechildren.net/">Save the Children</a> & <a href="http://www.cancerresearchuk.org/">Cancer Research UK</a>). It takes researchers less than 10 minutes to start data collection and many studies get completed in less than an hour. Here's a cool video created by one of our researchers, Andy Woods, explaining how Prolific works:<br></font></p>


<p style="text-align: center;"><iframe width="420" height="315" src="https://www.youtube.com/embed/eOlJCN7VnGQ?rel=0" frameborder="0" allowfullscreen></iframe></p>

<p style="text-align: center;">(Thanks, Andy!)</p>
<div class="embed-responsive embed-responsive-16by9">
	<p><font size="4">If you're looking for an alternative to MTurk and would like to give Prolific a try, then you can run your first £10 study <a href="https://prolificacademic.co.uk/rr?ref=5ZFZ276D">for free</a>. Plus, for a limited time, you can choose what commission you pay us. It can be as low as 0%. It's totally up to you!</font></p>
</div>